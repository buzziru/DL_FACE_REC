{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQCXYegJuf8n",
        "outputId": "30e35cc0-0b4a-404c-f6df-f7b5f293a862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4JeK62MzmhT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import torchsummary\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3Aej2u0R-aR"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)  # Python random 모듈\n",
        "    np.random.seed(seed)  # Numpy 랜덤 시드\n",
        "    torch.manual_seed(seed)  # PyTorch 랜덤 시드\n",
        "    torch.cuda.manual_seed(seed)  # GPU를 위한 PyTorch 랜덤 시드\n",
        "    torch.cuda.manual_seed_all(seed)  # 멀티 GPU를 위한 PyTorch 랜덤 시드\n",
        "\n",
        "seed_everything()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터셋 정의"
      ],
      "metadata": {
        "id": "Hnld49E7SBsI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqLZA6YqaVI1"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hdf5, transform=None):\n",
        "        self.transform = transform\n",
        "        self.file_object = h5py.File(hdf5, 'r')\n",
        "        self.keys = list(self.file_object['image'].keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        key = self.keys[index]\n",
        "        image = self.file_object['image'][key][:]\n",
        "        age = self.file_object['age'][key][()]\n",
        "        box = self.file_object['box'][key][()]\n",
        "\n",
        "        # 먼저 어레이를 PIL 이미지로 변환\n",
        "        image = Image.fromarray(image)\n",
        "        image = image.crop((box[0], box[1], box[0] + box[2], box[1] + box[3]))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = {'age_past': age}\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def close(self):\n",
        "        self.file_object.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 40대 이상 데이터 오버샘플링 - 비율 역수 가중치 부여\n",
        "class OverSampleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hdf5, transform=None):\n",
        "        self.transform = transform\n",
        "        self.file_object = h5py.File(hdf5, 'r')\n",
        "        self.keys = list(self.file_object['image'].keys())\n",
        "\n",
        "        # 연령대별 키 분류\n",
        "        self.age_groups = {\n",
        "            'below_60': [],\n",
        "            # '40s': [],\n",
        "            # '50s': [],\n",
        "            '60s_plus': []\n",
        "        }\n",
        "        for key in self.keys:\n",
        "            age = self.file_object['age'][key][()]\n",
        "            if age < 60:\n",
        "                self.age_groups['below_60'].append(key)\n",
        "            # elif age < 50:\n",
        "            #     self.age_groups['40s'].append(key)\n",
        "            # elif age < 60:\n",
        "                # self.age_groups['50s'].append(key)\n",
        "            else:\n",
        "                self.age_groups['60s_plus'].append(key)\n",
        "\n",
        "        # 오버 샘플링 비율 계산 및 적용\n",
        "        over_sampled_keys = self.age_groups['below_60']\n",
        "        for group, keys in self.age_groups.items():\n",
        "            if group == 'below_60':\n",
        "                continue\n",
        "            # oversample_rate = max(1, len(over_sampled_keys) / len(keys))  # 비율의 역수 계산, 최소 1배는 유지\n",
        "            oversample_rate = 4\n",
        "            oversampled_keys = np.random.choice(keys, int(len(keys) * oversample_rate), replace=True).tolist()\n",
        "            over_sampled_keys.extend(oversampled_keys)  # 오버샘플링된 키 추가\n",
        "\n",
        "        self.over_sampled_keys = over_sampled_keys\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.over_sampled_keys)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        key = self.over_sampled_keys[index]\n",
        "        image = self.file_object['image'][key][:]\n",
        "        age = self.file_object['age'][key][()]\n",
        "        box = self.file_object['box'][key][()]\n",
        "\n",
        "        image = Image.fromarray(image)\n",
        "        image = image.crop((box[0], box[1], box[0] + box[2], box[1] + box[3]))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = {'age_past': age}\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def close(self):\n",
        "        self.file_object.close()\n"
      ],
      "metadata": {
        "id": "sMPlW5zOKbdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 랜덤하게 적용\n",
        "random_transforms = transforms.RandomApply([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)\n",
        "], p=0.7)\n",
        "\n",
        "# 항상 적용\n",
        "always_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.6284, 0.4901, 0.4325], std=[0.1869, 0.1712, 0.1561]),\n",
        "])\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    random_transforms,\n",
        "    always_transforms\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    always_transforms\n",
        "])"
      ],
      "metadata": {
        "id": "DFEVxc113Vpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bojhHygkztrp"
      },
      "outputs": [],
      "source": [
        "train_dataset = OverSampleDataset('/content/drive/MyDrive/DL_DATA/DL_Face_REC/data_age_train.hdf5', transform=transform_train)\n",
        "val_dataset = CustomDataset('/content/drive/MyDrive/DL_DATA/DL_Face_REC/data_age_val.hdf5', transform=transform_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvBmQ0JM9Hmd"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    prefetch_factor=8\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2,\n",
        "    prefetch_factor=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWizzKPR38KV",
        "outputId": "ef6cd45c-f0b5-4944-b601-648c20cabf78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # DataLoader에서 단일 배치 테스트\n",
        "# def test_single_batch(data_loader):\n",
        "#     for data, labels in data_loader:\n",
        "#         print(\"Data shape:\", data.shape)\n",
        "#         print(\"Labels shape:\", labels['age_past'].shape)\n",
        "#         break\n",
        "\n",
        "# # DataLoader 전체 테스트\n",
        "# def test_full_loader(data_loader):\n",
        "#     for i, (data, labels) in enumerate(data_loader):\n",
        "#         if i % 10 == 0:\n",
        "#             print(f\"Batch {i}, Data shape: {data.shape}, Labels shape: {labels['age_past'].shape}\")\n",
        "\n",
        "# # val_loader 테스트 실행\n",
        "# print(\"Testing single batch from val_loader:\")\n",
        "# test_single_batch(val_loader)\n",
        "\n",
        "# print(\"\\nTesting full val_loader:\")\n",
        "# test_full_loader(val_loader)"
      ],
      "metadata": {
        "id": "mPK80cE28zg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3wCDUbTQ9OF",
        "outputId": "90e4cce8-cb50-4be4-d22c-159afd47939a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 - Inception ResNet v1\n",
        "- 사전 학습 모델 사용\n",
        "- 손실함수 MSE -> MAE"
      ],
      "metadata": {
        "id": "ZeoJNJNJSGjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvEml3iRyUBP",
        "outputId": "41057f91-6f64-4703-f651-2b04916363b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.31.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.16.0+cu121)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2024.2.2)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->facenet-pytorch) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision->facenet-pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision->facenet-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision->facenet-pytorch) (1.3.0)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from facenet_pytorch import InceptionResnetV1\n",
        "\n",
        "def make_age_model():\n",
        "    model = InceptionResnetV1(classify=True, pretrained='vggface2', num_classes=1, device=device)\n",
        "    in_dim = model.logits.in_features\n",
        "    model.logits = nn.Linear(in_dim, 1)\n",
        "    # 상단의 파라미터를 프리즈\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # (block8) 부분의 파라미터만 파인튜닝을 위해 언프리즈\n",
        "    for param in model.block8.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.last_linear.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.last_bn.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.logits.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    return model\n",
        "\n",
        "age_model = make_age_model().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7b9aad6db5e44a7293c6ef629d456df2",
            "b6f09a8808704103b7bf553ec4e5f680",
            "1104a548176848beba648390d91b744a",
            "93bf54e48f6542c3961699d9e3b7ec43",
            "f531ac6be7f44c709ae8712e2adee8d3",
            "3a9841e982b6468e97be7691f1189d08",
            "4380e2583bce4f21bfb861be063599c4",
            "c806d0320d0e45b2b23f7c4c0c496f89",
            "e19f958bf8124f348e4242aababe21f6",
            "8959167962ff4978b0ff74c83447b870",
            "39a9da5ab0bf4580bd7db989b2052d02"
          ]
        },
        "id": "vrgTqGoRR_m3",
        "outputId": "59ec17ad-6a9d-41fa-d6c0-59f4b6a5c5a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b9aad6db5e44a7293c6ef629d456df2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torchsummary.summary(age_model, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10AV2eBgSzDR",
        "outputId": "3d7a12e9-57aa-4f18-8b08-5e42ab5488d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 111, 111]             864\n",
            "       BatchNorm2d-2         [-1, 32, 111, 111]              64\n",
            "              ReLU-3         [-1, 32, 111, 111]               0\n",
            "       BasicConv2d-4         [-1, 32, 111, 111]               0\n",
            "            Conv2d-5         [-1, 32, 109, 109]           9,216\n",
            "       BatchNorm2d-6         [-1, 32, 109, 109]              64\n",
            "              ReLU-7         [-1, 32, 109, 109]               0\n",
            "       BasicConv2d-8         [-1, 32, 109, 109]               0\n",
            "            Conv2d-9         [-1, 64, 109, 109]          18,432\n",
            "      BatchNorm2d-10         [-1, 64, 109, 109]             128\n",
            "             ReLU-11         [-1, 64, 109, 109]               0\n",
            "      BasicConv2d-12         [-1, 64, 109, 109]               0\n",
            "        MaxPool2d-13           [-1, 64, 54, 54]               0\n",
            "           Conv2d-14           [-1, 80, 54, 54]           5,120\n",
            "      BatchNorm2d-15           [-1, 80, 54, 54]             160\n",
            "             ReLU-16           [-1, 80, 54, 54]               0\n",
            "      BasicConv2d-17           [-1, 80, 54, 54]               0\n",
            "           Conv2d-18          [-1, 192, 52, 52]         138,240\n",
            "      BatchNorm2d-19          [-1, 192, 52, 52]             384\n",
            "             ReLU-20          [-1, 192, 52, 52]               0\n",
            "      BasicConv2d-21          [-1, 192, 52, 52]               0\n",
            "           Conv2d-22          [-1, 256, 25, 25]         442,368\n",
            "      BatchNorm2d-23          [-1, 256, 25, 25]             512\n",
            "             ReLU-24          [-1, 256, 25, 25]               0\n",
            "      BasicConv2d-25          [-1, 256, 25, 25]               0\n",
            "           Conv2d-26           [-1, 32, 25, 25]           8,192\n",
            "      BatchNorm2d-27           [-1, 32, 25, 25]              64\n",
            "             ReLU-28           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-29           [-1, 32, 25, 25]               0\n",
            "           Conv2d-30           [-1, 32, 25, 25]           8,192\n",
            "      BatchNorm2d-31           [-1, 32, 25, 25]              64\n",
            "             ReLU-32           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-33           [-1, 32, 25, 25]               0\n",
            "           Conv2d-34           [-1, 32, 25, 25]           9,216\n",
            "      BatchNorm2d-35           [-1, 32, 25, 25]              64\n",
            "             ReLU-36           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-37           [-1, 32, 25, 25]               0\n",
            "           Conv2d-38           [-1, 32, 25, 25]           8,192\n",
            "      BatchNorm2d-39           [-1, 32, 25, 25]              64\n",
            "             ReLU-40           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-41           [-1, 32, 25, 25]               0\n",
            "           Conv2d-42           [-1, 32, 25, 25]           9,216\n",
            "      BatchNorm2d-43           [-1, 32, 25, 25]              64\n",
            "             ReLU-44           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-45           [-1, 32, 25, 25]               0\n",
            "           Conv2d-46           [-1, 32, 25, 25]           9,216\n",
            "      BatchNorm2d-47           [-1, 32, 25, 25]              64\n",
            "             ReLU-48           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-49           [-1, 32, 25, 25]               0\n",
            "           Conv2d-50          [-1, 256, 25, 25]          24,832\n",
            "             ReLU-51          [-1, 256, 25, 25]               0\n",
            "          Block35-52          [-1, 256, 25, 25]               0\n",
            "           Conv2d-53           [-1, 32, 25, 25]           8,192\n",
            "      BatchNorm2d-54           [-1, 32, 25, 25]              64\n",
            "             ReLU-55           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-56           [-1, 32, 25, 25]               0\n",
            "           Conv2d-57           [-1, 32, 25, 25]           8,192\n",
            "      BatchNorm2d-58           [-1, 32, 25, 25]              64\n",
            "             ReLU-59           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-60           [-1, 32, 25, 25]               0\n",
            "           Conv2d-61           [-1, 32, 25, 25]           9,216\n",
            "      BatchNorm2d-62           [-1, 32, 25, 25]              64\n",
            "             ReLU-63           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-64           [-1, 32, 25, 25]               0\n",
            "           Conv2d-65           [-1, 32, 25, 25]           8,192\n",
            "      BatchNorm2d-66           [-1, 32, 25, 25]              64\n",
            "             ReLU-67           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-68           [-1, 32, 25, 25]               0\n",
            "           Conv2d-69           [-1, 32, 25, 25]           9,216\n",
            "      BatchNorm2d-70           [-1, 32, 25, 25]              64\n",
            "             ReLU-71           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-72           [-1, 32, 25, 25]               0\n",
            "           Conv2d-73           [-1, 32, 25, 25]           9,216\n",
            "      BatchNorm2d-74           [-1, 32, 25, 25]              64\n",
            "             ReLU-75           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-76           [-1, 32, 25, 25]               0\n",
            "           Conv2d-77          [-1, 256, 25, 25]          24,832\n",
            "             ReLU-78          [-1, 256, 25, 25]               0\n",
            "          Block35-79          [-1, 256, 25, 25]               0\n",
            "           Conv2d-80           [-1, 32, 25, 25]           8,192\n",
            "      BatchNorm2d-81           [-1, 32, 25, 25]              64\n",
            "             ReLU-82           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-83           [-1, 32, 25, 25]               0\n",
            "           Conv2d-84           [-1, 32, 25, 25]           8,192\n",
            "      BatchNorm2d-85           [-1, 32, 25, 25]              64\n",
            "             ReLU-86           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-87           [-1, 32, 25, 25]               0\n",
            "           Conv2d-88           [-1, 32, 25, 25]           9,216\n",
            "      BatchNorm2d-89           [-1, 32, 25, 25]              64\n",
            "             ReLU-90           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-91           [-1, 32, 25, 25]               0\n",
            "           Conv2d-92           [-1, 32, 25, 25]           8,192\n",
            "      BatchNorm2d-93           [-1, 32, 25, 25]              64\n",
            "             ReLU-94           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-95           [-1, 32, 25, 25]               0\n",
            "           Conv2d-96           [-1, 32, 25, 25]           9,216\n",
            "      BatchNorm2d-97           [-1, 32, 25, 25]              64\n",
            "             ReLU-98           [-1, 32, 25, 25]               0\n",
            "      BasicConv2d-99           [-1, 32, 25, 25]               0\n",
            "          Conv2d-100           [-1, 32, 25, 25]           9,216\n",
            "     BatchNorm2d-101           [-1, 32, 25, 25]              64\n",
            "            ReLU-102           [-1, 32, 25, 25]               0\n",
            "     BasicConv2d-103           [-1, 32, 25, 25]               0\n",
            "          Conv2d-104          [-1, 256, 25, 25]          24,832\n",
            "            ReLU-105          [-1, 256, 25, 25]               0\n",
            "         Block35-106          [-1, 256, 25, 25]               0\n",
            "          Conv2d-107           [-1, 32, 25, 25]           8,192\n",
            "     BatchNorm2d-108           [-1, 32, 25, 25]              64\n",
            "            ReLU-109           [-1, 32, 25, 25]               0\n",
            "     BasicConv2d-110           [-1, 32, 25, 25]               0\n",
            "          Conv2d-111           [-1, 32, 25, 25]           8,192\n",
            "     BatchNorm2d-112           [-1, 32, 25, 25]              64\n",
            "            ReLU-113           [-1, 32, 25, 25]               0\n",
            "     BasicConv2d-114           [-1, 32, 25, 25]               0\n",
            "          Conv2d-115           [-1, 32, 25, 25]           9,216\n",
            "     BatchNorm2d-116           [-1, 32, 25, 25]              64\n",
            "            ReLU-117           [-1, 32, 25, 25]               0\n",
            "     BasicConv2d-118           [-1, 32, 25, 25]               0\n",
            "          Conv2d-119           [-1, 32, 25, 25]           8,192\n",
            "     BatchNorm2d-120           [-1, 32, 25, 25]              64\n",
            "            ReLU-121           [-1, 32, 25, 25]               0\n",
            "     BasicConv2d-122           [-1, 32, 25, 25]               0\n",
            "          Conv2d-123           [-1, 32, 25, 25]           9,216\n",
            "     BatchNorm2d-124           [-1, 32, 25, 25]              64\n",
            "            ReLU-125           [-1, 32, 25, 25]               0\n",
            "     BasicConv2d-126           [-1, 32, 25, 25]               0\n",
            "          Conv2d-127           [-1, 32, 25, 25]           9,216\n",
            "     BatchNorm2d-128           [-1, 32, 25, 25]              64\n",
            "            ReLU-129           [-1, 32, 25, 25]               0\n",
            "     BasicConv2d-130           [-1, 32, 25, 25]               0\n",
            "          Conv2d-131          [-1, 256, 25, 25]          24,832\n",
            "            ReLU-132          [-1, 256, 25, 25]               0\n",
            "         Block35-133          [-1, 256, 25, 25]               0\n",
            "          Conv2d-134           [-1, 32, 25, 25]           8,192\n",
            "     BatchNorm2d-135           [-1, 32, 25, 25]              64\n",
            "            ReLU-136           [-1, 32, 25, 25]               0\n",
            "     BasicConv2d-137           [-1, 32, 25, 25]               0\n",
            "          Conv2d-138           [-1, 32, 25, 25]           8,192\n",
            "     BatchNorm2d-139           [-1, 32, 25, 25]              64\n",
            "            ReLU-140           [-1, 32, 25, 25]               0\n",
            "     BasicConv2d-141           [-1, 32, 25, 25]               0\n",
            "          Conv2d-142           [-1, 32, 25, 25]           9,216\n",
            "     BatchNorm2d-143           [-1, 32, 25, 25]              64\n",
            "            ReLU-144           [-1, 32, 25, 25]               0\n",
            "     BasicConv2d-145           [-1, 32, 25, 25]               0\n",
            "          Conv2d-146           [-1, 32, 25, 25]           8,192\n",
            "     BatchNorm2d-147           [-1, 32, 25, 25]              64\n",
            "            ReLU-148           [-1, 32, 25, 25]               0\n",
            "     BasicConv2d-149           [-1, 32, 25, 25]               0\n",
            "          Conv2d-150           [-1, 32, 25, 25]           9,216\n",
            "     BatchNorm2d-151           [-1, 32, 25, 25]              64\n",
            "            ReLU-152           [-1, 32, 25, 25]               0\n",
            "     BasicConv2d-153           [-1, 32, 25, 25]               0\n",
            "          Conv2d-154           [-1, 32, 25, 25]           9,216\n",
            "     BatchNorm2d-155           [-1, 32, 25, 25]              64\n",
            "            ReLU-156           [-1, 32, 25, 25]               0\n",
            "     BasicConv2d-157           [-1, 32, 25, 25]               0\n",
            "          Conv2d-158          [-1, 256, 25, 25]          24,832\n",
            "            ReLU-159          [-1, 256, 25, 25]               0\n",
            "         Block35-160          [-1, 256, 25, 25]               0\n",
            "          Conv2d-161          [-1, 384, 12, 12]         884,736\n",
            "     BatchNorm2d-162          [-1, 384, 12, 12]             768\n",
            "            ReLU-163          [-1, 384, 12, 12]               0\n",
            "     BasicConv2d-164          [-1, 384, 12, 12]               0\n",
            "          Conv2d-165          [-1, 192, 25, 25]          49,152\n",
            "     BatchNorm2d-166          [-1, 192, 25, 25]             384\n",
            "            ReLU-167          [-1, 192, 25, 25]               0\n",
            "     BasicConv2d-168          [-1, 192, 25, 25]               0\n",
            "          Conv2d-169          [-1, 192, 25, 25]         331,776\n",
            "     BatchNorm2d-170          [-1, 192, 25, 25]             384\n",
            "            ReLU-171          [-1, 192, 25, 25]               0\n",
            "     BasicConv2d-172          [-1, 192, 25, 25]               0\n",
            "          Conv2d-173          [-1, 256, 12, 12]         442,368\n",
            "     BatchNorm2d-174          [-1, 256, 12, 12]             512\n",
            "            ReLU-175          [-1, 256, 12, 12]               0\n",
            "     BasicConv2d-176          [-1, 256, 12, 12]               0\n",
            "       MaxPool2d-177          [-1, 256, 12, 12]               0\n",
            "        Mixed_6a-178          [-1, 896, 12, 12]               0\n",
            "          Conv2d-179          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-180          [-1, 128, 12, 12]             256\n",
            "            ReLU-181          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-182          [-1, 128, 12, 12]               0\n",
            "          Conv2d-183          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-184          [-1, 128, 12, 12]             256\n",
            "            ReLU-185          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-186          [-1, 128, 12, 12]               0\n",
            "          Conv2d-187          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-188          [-1, 128, 12, 12]             256\n",
            "            ReLU-189          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-190          [-1, 128, 12, 12]               0\n",
            "          Conv2d-191          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-192          [-1, 128, 12, 12]             256\n",
            "            ReLU-193          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-194          [-1, 128, 12, 12]               0\n",
            "          Conv2d-195          [-1, 896, 12, 12]         230,272\n",
            "            ReLU-196          [-1, 896, 12, 12]               0\n",
            "         Block17-197          [-1, 896, 12, 12]               0\n",
            "          Conv2d-198          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-199          [-1, 128, 12, 12]             256\n",
            "            ReLU-200          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-201          [-1, 128, 12, 12]               0\n",
            "          Conv2d-202          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-203          [-1, 128, 12, 12]             256\n",
            "            ReLU-204          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-205          [-1, 128, 12, 12]               0\n",
            "          Conv2d-206          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-207          [-1, 128, 12, 12]             256\n",
            "            ReLU-208          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-209          [-1, 128, 12, 12]               0\n",
            "          Conv2d-210          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-211          [-1, 128, 12, 12]             256\n",
            "            ReLU-212          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-213          [-1, 128, 12, 12]               0\n",
            "          Conv2d-214          [-1, 896, 12, 12]         230,272\n",
            "            ReLU-215          [-1, 896, 12, 12]               0\n",
            "         Block17-216          [-1, 896, 12, 12]               0\n",
            "          Conv2d-217          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-218          [-1, 128, 12, 12]             256\n",
            "            ReLU-219          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-220          [-1, 128, 12, 12]               0\n",
            "          Conv2d-221          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-222          [-1, 128, 12, 12]             256\n",
            "            ReLU-223          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-224          [-1, 128, 12, 12]               0\n",
            "          Conv2d-225          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-226          [-1, 128, 12, 12]             256\n",
            "            ReLU-227          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-228          [-1, 128, 12, 12]               0\n",
            "          Conv2d-229          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-230          [-1, 128, 12, 12]             256\n",
            "            ReLU-231          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-232          [-1, 128, 12, 12]               0\n",
            "          Conv2d-233          [-1, 896, 12, 12]         230,272\n",
            "            ReLU-234          [-1, 896, 12, 12]               0\n",
            "         Block17-235          [-1, 896, 12, 12]               0\n",
            "          Conv2d-236          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-237          [-1, 128, 12, 12]             256\n",
            "            ReLU-238          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-239          [-1, 128, 12, 12]               0\n",
            "          Conv2d-240          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-241          [-1, 128, 12, 12]             256\n",
            "            ReLU-242          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-243          [-1, 128, 12, 12]               0\n",
            "          Conv2d-244          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-245          [-1, 128, 12, 12]             256\n",
            "            ReLU-246          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-247          [-1, 128, 12, 12]               0\n",
            "          Conv2d-248          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-249          [-1, 128, 12, 12]             256\n",
            "            ReLU-250          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-251          [-1, 128, 12, 12]               0\n",
            "          Conv2d-252          [-1, 896, 12, 12]         230,272\n",
            "            ReLU-253          [-1, 896, 12, 12]               0\n",
            "         Block17-254          [-1, 896, 12, 12]               0\n",
            "          Conv2d-255          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-256          [-1, 128, 12, 12]             256\n",
            "            ReLU-257          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-258          [-1, 128, 12, 12]               0\n",
            "          Conv2d-259          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-260          [-1, 128, 12, 12]             256\n",
            "            ReLU-261          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-262          [-1, 128, 12, 12]               0\n",
            "          Conv2d-263          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-264          [-1, 128, 12, 12]             256\n",
            "            ReLU-265          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-266          [-1, 128, 12, 12]               0\n",
            "          Conv2d-267          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-268          [-1, 128, 12, 12]             256\n",
            "            ReLU-269          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-270          [-1, 128, 12, 12]               0\n",
            "          Conv2d-271          [-1, 896, 12, 12]         230,272\n",
            "            ReLU-272          [-1, 896, 12, 12]               0\n",
            "         Block17-273          [-1, 896, 12, 12]               0\n",
            "          Conv2d-274          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-275          [-1, 128, 12, 12]             256\n",
            "            ReLU-276          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-277          [-1, 128, 12, 12]               0\n",
            "          Conv2d-278          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-279          [-1, 128, 12, 12]             256\n",
            "            ReLU-280          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-281          [-1, 128, 12, 12]               0\n",
            "          Conv2d-282          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-283          [-1, 128, 12, 12]             256\n",
            "            ReLU-284          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-285          [-1, 128, 12, 12]               0\n",
            "          Conv2d-286          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-287          [-1, 128, 12, 12]             256\n",
            "            ReLU-288          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-289          [-1, 128, 12, 12]               0\n",
            "          Conv2d-290          [-1, 896, 12, 12]         230,272\n",
            "            ReLU-291          [-1, 896, 12, 12]               0\n",
            "         Block17-292          [-1, 896, 12, 12]               0\n",
            "          Conv2d-293          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-294          [-1, 128, 12, 12]             256\n",
            "            ReLU-295          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-296          [-1, 128, 12, 12]               0\n",
            "          Conv2d-297          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-298          [-1, 128, 12, 12]             256\n",
            "            ReLU-299          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-300          [-1, 128, 12, 12]               0\n",
            "          Conv2d-301          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-302          [-1, 128, 12, 12]             256\n",
            "            ReLU-303          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-304          [-1, 128, 12, 12]               0\n",
            "          Conv2d-305          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-306          [-1, 128, 12, 12]             256\n",
            "            ReLU-307          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-308          [-1, 128, 12, 12]               0\n",
            "          Conv2d-309          [-1, 896, 12, 12]         230,272\n",
            "            ReLU-310          [-1, 896, 12, 12]               0\n",
            "         Block17-311          [-1, 896, 12, 12]               0\n",
            "          Conv2d-312          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-313          [-1, 128, 12, 12]             256\n",
            "            ReLU-314          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-315          [-1, 128, 12, 12]               0\n",
            "          Conv2d-316          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-317          [-1, 128, 12, 12]             256\n",
            "            ReLU-318          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-319          [-1, 128, 12, 12]               0\n",
            "          Conv2d-320          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-321          [-1, 128, 12, 12]             256\n",
            "            ReLU-322          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-323          [-1, 128, 12, 12]               0\n",
            "          Conv2d-324          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-325          [-1, 128, 12, 12]             256\n",
            "            ReLU-326          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-327          [-1, 128, 12, 12]               0\n",
            "          Conv2d-328          [-1, 896, 12, 12]         230,272\n",
            "            ReLU-329          [-1, 896, 12, 12]               0\n",
            "         Block17-330          [-1, 896, 12, 12]               0\n",
            "          Conv2d-331          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-332          [-1, 128, 12, 12]             256\n",
            "            ReLU-333          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-334          [-1, 128, 12, 12]               0\n",
            "          Conv2d-335          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-336          [-1, 128, 12, 12]             256\n",
            "            ReLU-337          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-338          [-1, 128, 12, 12]               0\n",
            "          Conv2d-339          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-340          [-1, 128, 12, 12]             256\n",
            "            ReLU-341          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-342          [-1, 128, 12, 12]               0\n",
            "          Conv2d-343          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-344          [-1, 128, 12, 12]             256\n",
            "            ReLU-345          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-346          [-1, 128, 12, 12]               0\n",
            "          Conv2d-347          [-1, 896, 12, 12]         230,272\n",
            "            ReLU-348          [-1, 896, 12, 12]               0\n",
            "         Block17-349          [-1, 896, 12, 12]               0\n",
            "          Conv2d-350          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-351          [-1, 128, 12, 12]             256\n",
            "            ReLU-352          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-353          [-1, 128, 12, 12]               0\n",
            "          Conv2d-354          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-355          [-1, 128, 12, 12]             256\n",
            "            ReLU-356          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-357          [-1, 128, 12, 12]               0\n",
            "          Conv2d-358          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-359          [-1, 128, 12, 12]             256\n",
            "            ReLU-360          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-361          [-1, 128, 12, 12]               0\n",
            "          Conv2d-362          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-363          [-1, 128, 12, 12]             256\n",
            "            ReLU-364          [-1, 128, 12, 12]               0\n",
            "     BasicConv2d-365          [-1, 128, 12, 12]               0\n",
            "          Conv2d-366          [-1, 896, 12, 12]         230,272\n",
            "            ReLU-367          [-1, 896, 12, 12]               0\n",
            "         Block17-368          [-1, 896, 12, 12]               0\n",
            "          Conv2d-369          [-1, 256, 12, 12]         229,376\n",
            "     BatchNorm2d-370          [-1, 256, 12, 12]             512\n",
            "            ReLU-371          [-1, 256, 12, 12]               0\n",
            "     BasicConv2d-372          [-1, 256, 12, 12]               0\n",
            "          Conv2d-373            [-1, 384, 5, 5]         884,736\n",
            "     BatchNorm2d-374            [-1, 384, 5, 5]             768\n",
            "            ReLU-375            [-1, 384, 5, 5]               0\n",
            "     BasicConv2d-376            [-1, 384, 5, 5]               0\n",
            "          Conv2d-377          [-1, 256, 12, 12]         229,376\n",
            "     BatchNorm2d-378          [-1, 256, 12, 12]             512\n",
            "            ReLU-379          [-1, 256, 12, 12]               0\n",
            "     BasicConv2d-380          [-1, 256, 12, 12]               0\n",
            "          Conv2d-381            [-1, 256, 5, 5]         589,824\n",
            "     BatchNorm2d-382            [-1, 256, 5, 5]             512\n",
            "            ReLU-383            [-1, 256, 5, 5]               0\n",
            "     BasicConv2d-384            [-1, 256, 5, 5]               0\n",
            "          Conv2d-385          [-1, 256, 12, 12]         229,376\n",
            "     BatchNorm2d-386          [-1, 256, 12, 12]             512\n",
            "            ReLU-387          [-1, 256, 12, 12]               0\n",
            "     BasicConv2d-388          [-1, 256, 12, 12]               0\n",
            "          Conv2d-389          [-1, 256, 12, 12]         589,824\n",
            "     BatchNorm2d-390          [-1, 256, 12, 12]             512\n",
            "            ReLU-391          [-1, 256, 12, 12]               0\n",
            "     BasicConv2d-392          [-1, 256, 12, 12]               0\n",
            "          Conv2d-393            [-1, 256, 5, 5]         589,824\n",
            "     BatchNorm2d-394            [-1, 256, 5, 5]             512\n",
            "            ReLU-395            [-1, 256, 5, 5]               0\n",
            "     BasicConv2d-396            [-1, 256, 5, 5]               0\n",
            "       MaxPool2d-397            [-1, 896, 5, 5]               0\n",
            "        Mixed_7a-398           [-1, 1792, 5, 5]               0\n",
            "          Conv2d-399            [-1, 192, 5, 5]         344,064\n",
            "     BatchNorm2d-400            [-1, 192, 5, 5]             384\n",
            "            ReLU-401            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-402            [-1, 192, 5, 5]               0\n",
            "          Conv2d-403            [-1, 192, 5, 5]         344,064\n",
            "     BatchNorm2d-404            [-1, 192, 5, 5]             384\n",
            "            ReLU-405            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-406            [-1, 192, 5, 5]               0\n",
            "          Conv2d-407            [-1, 192, 5, 5]         110,592\n",
            "     BatchNorm2d-408            [-1, 192, 5, 5]             384\n",
            "            ReLU-409            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-410            [-1, 192, 5, 5]               0\n",
            "          Conv2d-411            [-1, 192, 5, 5]         110,592\n",
            "     BatchNorm2d-412            [-1, 192, 5, 5]             384\n",
            "            ReLU-413            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-414            [-1, 192, 5, 5]               0\n",
            "          Conv2d-415           [-1, 1792, 5, 5]         689,920\n",
            "            ReLU-416           [-1, 1792, 5, 5]               0\n",
            "          Block8-417           [-1, 1792, 5, 5]               0\n",
            "          Conv2d-418            [-1, 192, 5, 5]         344,064\n",
            "     BatchNorm2d-419            [-1, 192, 5, 5]             384\n",
            "            ReLU-420            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-421            [-1, 192, 5, 5]               0\n",
            "          Conv2d-422            [-1, 192, 5, 5]         344,064\n",
            "     BatchNorm2d-423            [-1, 192, 5, 5]             384\n",
            "            ReLU-424            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-425            [-1, 192, 5, 5]               0\n",
            "          Conv2d-426            [-1, 192, 5, 5]         110,592\n",
            "     BatchNorm2d-427            [-1, 192, 5, 5]             384\n",
            "            ReLU-428            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-429            [-1, 192, 5, 5]               0\n",
            "          Conv2d-430            [-1, 192, 5, 5]         110,592\n",
            "     BatchNorm2d-431            [-1, 192, 5, 5]             384\n",
            "            ReLU-432            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-433            [-1, 192, 5, 5]               0\n",
            "          Conv2d-434           [-1, 1792, 5, 5]         689,920\n",
            "            ReLU-435           [-1, 1792, 5, 5]               0\n",
            "          Block8-436           [-1, 1792, 5, 5]               0\n",
            "          Conv2d-437            [-1, 192, 5, 5]         344,064\n",
            "     BatchNorm2d-438            [-1, 192, 5, 5]             384\n",
            "            ReLU-439            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-440            [-1, 192, 5, 5]               0\n",
            "          Conv2d-441            [-1, 192, 5, 5]         344,064\n",
            "     BatchNorm2d-442            [-1, 192, 5, 5]             384\n",
            "            ReLU-443            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-444            [-1, 192, 5, 5]               0\n",
            "          Conv2d-445            [-1, 192, 5, 5]         110,592\n",
            "     BatchNorm2d-446            [-1, 192, 5, 5]             384\n",
            "            ReLU-447            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-448            [-1, 192, 5, 5]               0\n",
            "          Conv2d-449            [-1, 192, 5, 5]         110,592\n",
            "     BatchNorm2d-450            [-1, 192, 5, 5]             384\n",
            "            ReLU-451            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-452            [-1, 192, 5, 5]               0\n",
            "          Conv2d-453           [-1, 1792, 5, 5]         689,920\n",
            "            ReLU-454           [-1, 1792, 5, 5]               0\n",
            "          Block8-455           [-1, 1792, 5, 5]               0\n",
            "          Conv2d-456            [-1, 192, 5, 5]         344,064\n",
            "     BatchNorm2d-457            [-1, 192, 5, 5]             384\n",
            "            ReLU-458            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-459            [-1, 192, 5, 5]               0\n",
            "          Conv2d-460            [-1, 192, 5, 5]         344,064\n",
            "     BatchNorm2d-461            [-1, 192, 5, 5]             384\n",
            "            ReLU-462            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-463            [-1, 192, 5, 5]               0\n",
            "          Conv2d-464            [-1, 192, 5, 5]         110,592\n",
            "     BatchNorm2d-465            [-1, 192, 5, 5]             384\n",
            "            ReLU-466            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-467            [-1, 192, 5, 5]               0\n",
            "          Conv2d-468            [-1, 192, 5, 5]         110,592\n",
            "     BatchNorm2d-469            [-1, 192, 5, 5]             384\n",
            "            ReLU-470            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-471            [-1, 192, 5, 5]               0\n",
            "          Conv2d-472           [-1, 1792, 5, 5]         689,920\n",
            "            ReLU-473           [-1, 1792, 5, 5]               0\n",
            "          Block8-474           [-1, 1792, 5, 5]               0\n",
            "          Conv2d-475            [-1, 192, 5, 5]         344,064\n",
            "     BatchNorm2d-476            [-1, 192, 5, 5]             384\n",
            "            ReLU-477            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-478            [-1, 192, 5, 5]               0\n",
            "          Conv2d-479            [-1, 192, 5, 5]         344,064\n",
            "     BatchNorm2d-480            [-1, 192, 5, 5]             384\n",
            "            ReLU-481            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-482            [-1, 192, 5, 5]               0\n",
            "          Conv2d-483            [-1, 192, 5, 5]         110,592\n",
            "     BatchNorm2d-484            [-1, 192, 5, 5]             384\n",
            "            ReLU-485            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-486            [-1, 192, 5, 5]               0\n",
            "          Conv2d-487            [-1, 192, 5, 5]         110,592\n",
            "     BatchNorm2d-488            [-1, 192, 5, 5]             384\n",
            "            ReLU-489            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-490            [-1, 192, 5, 5]               0\n",
            "          Conv2d-491           [-1, 1792, 5, 5]         689,920\n",
            "            ReLU-492           [-1, 1792, 5, 5]               0\n",
            "          Block8-493           [-1, 1792, 5, 5]               0\n",
            "          Conv2d-494            [-1, 192, 5, 5]         344,064\n",
            "     BatchNorm2d-495            [-1, 192, 5, 5]             384\n",
            "            ReLU-496            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-497            [-1, 192, 5, 5]               0\n",
            "          Conv2d-498            [-1, 192, 5, 5]         344,064\n",
            "     BatchNorm2d-499            [-1, 192, 5, 5]             384\n",
            "            ReLU-500            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-501            [-1, 192, 5, 5]               0\n",
            "          Conv2d-502            [-1, 192, 5, 5]         110,592\n",
            "     BatchNorm2d-503            [-1, 192, 5, 5]             384\n",
            "            ReLU-504            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-505            [-1, 192, 5, 5]               0\n",
            "          Conv2d-506            [-1, 192, 5, 5]         110,592\n",
            "     BatchNorm2d-507            [-1, 192, 5, 5]             384\n",
            "            ReLU-508            [-1, 192, 5, 5]               0\n",
            "     BasicConv2d-509            [-1, 192, 5, 5]               0\n",
            "          Conv2d-510           [-1, 1792, 5, 5]         689,920\n",
            "          Block8-511           [-1, 1792, 5, 5]               0\n",
            "AdaptiveAvgPool2d-512           [-1, 1792, 1, 1]               0\n",
            "         Dropout-513           [-1, 1792, 1, 1]               0\n",
            "          Linear-514                  [-1, 512]         917,504\n",
            "     BatchNorm1d-515                  [-1, 512]           1,024\n",
            "          Linear-516                    [-1, 1]             513\n",
            "================================================================\n",
            "Total params: 23,483,137\n",
            "Trainable params: 2,519,809\n",
            "Non-trainable params: 20,963,328\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 191.22\n",
            "Params size (MB): 89.58\n",
            "Estimated Total Size (MB): 281.38\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습과정"
      ],
      "metadata": {
        "id": "UoBZvysESLTG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujNm-g_RO3r0"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.optim as optim\n",
        "\n",
        "opt_age = optim.Adam(age_model.parameters(), lr=0.0003)\n",
        "age_lr_scheduler = ReduceLROnPlateau(opt_age, mode='min', verbose=True, cooldown=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx8_usRMTCus"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.delta = delta\n",
        "        self.best_loss = np.inf\n",
        "        self.early_stop = False\n",
        "        self.counter = 0\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss - val_loss > self.delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                if self.verbose:\n",
        "                    print(\"Early stopping\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndXBKj_zTTVB"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWMH8jaiQZ_6"
      },
      "outputs": [],
      "source": [
        "def train_loop_age(dataloader, model, loss_fn, optimizer, epoch):\n",
        "    model.train()\n",
        "    size = len(dataloader.dataset)\n",
        "    total_loss = 0.0\n",
        "    start_time = time.time()  # 에포크 시작 시간\n",
        "\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        batch_start_time = time.time()  # 배치 처리 시작 시간\n",
        "        x, y = x.to(device), y['age_past'].float().to(device)\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred.squeeze(), y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            batch_process_time = time.time() - batch_start_time\n",
        "            processed = (batch + 1) * len(x)\n",
        "            print(f'Epoch {epoch+1} : [{processed} / {size}] loss : {loss.item()}, Batch time: {batch_process_time:.4f} sec')\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Finished, Total Epoch time: {epoch_time:.5f} sec, Train Loss: {average_loss:.5f}\")\n",
        "    return average_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLUS456MTlUU"
      },
      "outputs": [],
      "source": [
        "def validation_loop_age(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y['age_past'].float().to(device)\n",
        "            pred = model(x)\n",
        "            loss = loss_fn(pred.squeeze(), y)\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(dataloader)\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DU8WjVus4Iw"
      },
      "outputs": [],
      "source": [
        "def save_model(epoch, model, optimizer, path, train_loss, val_loss, scheduler):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'scheduler': scheduler.state_dict() # 스케쥴러도 함께 저장\n",
        "    }, path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aeckk9XphyWR",
        "outputId": "66b3c90d-7fde-47e7-8a7a-2f322cc1d620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(age_model, input_size=(256, 3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxiJfoCYhrfu",
        "outputId": "f3b5f93f-28d5-4c56-aaab-2e3613100c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "InceptionResnetV1                             [256, 1]                  --\n",
              "├─BasicConv2d: 1-1                            [256, 32, 111, 111]       --\n",
              "│    └─Conv2d: 2-1                            [256, 32, 111, 111]       (864)\n",
              "│    └─BatchNorm2d: 2-2                       [256, 32, 111, 111]       (64)\n",
              "│    └─ReLU: 2-3                              [256, 32, 111, 111]       --\n",
              "├─BasicConv2d: 1-2                            [256, 32, 109, 109]       --\n",
              "│    └─Conv2d: 2-4                            [256, 32, 109, 109]       (9,216)\n",
              "│    └─BatchNorm2d: 2-5                       [256, 32, 109, 109]       (64)\n",
              "│    └─ReLU: 2-6                              [256, 32, 109, 109]       --\n",
              "├─BasicConv2d: 1-3                            [256, 64, 109, 109]       --\n",
              "│    └─Conv2d: 2-7                            [256, 64, 109, 109]       (18,432)\n",
              "│    └─BatchNorm2d: 2-8                       [256, 64, 109, 109]       (128)\n",
              "│    └─ReLU: 2-9                              [256, 64, 109, 109]       --\n",
              "├─MaxPool2d: 1-4                              [256, 64, 54, 54]         --\n",
              "├─BasicConv2d: 1-5                            [256, 80, 54, 54]         --\n",
              "│    └─Conv2d: 2-10                           [256, 80, 54, 54]         (5,120)\n",
              "│    └─BatchNorm2d: 2-11                      [256, 80, 54, 54]         (160)\n",
              "│    └─ReLU: 2-12                             [256, 80, 54, 54]         --\n",
              "├─BasicConv2d: 1-6                            [256, 192, 52, 52]        --\n",
              "│    └─Conv2d: 2-13                           [256, 192, 52, 52]        (138,240)\n",
              "│    └─BatchNorm2d: 2-14                      [256, 192, 52, 52]        (384)\n",
              "│    └─ReLU: 2-15                             [256, 192, 52, 52]        --\n",
              "├─BasicConv2d: 1-7                            [256, 256, 25, 25]        --\n",
              "│    └─Conv2d: 2-16                           [256, 256, 25, 25]        (442,368)\n",
              "│    └─BatchNorm2d: 2-17                      [256, 256, 25, 25]        (512)\n",
              "│    └─ReLU: 2-18                             [256, 256, 25, 25]        --\n",
              "├─Sequential: 1-8                             [256, 256, 25, 25]        --\n",
              "│    └─Block35: 2-19                          [256, 256, 25, 25]        --\n",
              "│    │    └─BasicConv2d: 3-1                  [256, 32, 25, 25]         (8,256)\n",
              "│    │    └─Sequential: 3-2                   [256, 32, 25, 25]         (17,536)\n",
              "│    │    └─Sequential: 3-3                   [256, 32, 25, 25]         (26,816)\n",
              "│    │    └─Conv2d: 3-4                       [256, 256, 25, 25]        (24,832)\n",
              "│    │    └─ReLU: 3-5                         [256, 256, 25, 25]        --\n",
              "│    └─Block35: 2-20                          [256, 256, 25, 25]        --\n",
              "│    │    └─BasicConv2d: 3-6                  [256, 32, 25, 25]         (8,256)\n",
              "│    │    └─Sequential: 3-7                   [256, 32, 25, 25]         (17,536)\n",
              "│    │    └─Sequential: 3-8                   [256, 32, 25, 25]         (26,816)\n",
              "│    │    └─Conv2d: 3-9                       [256, 256, 25, 25]        (24,832)\n",
              "│    │    └─ReLU: 3-10                        [256, 256, 25, 25]        --\n",
              "│    └─Block35: 2-21                          [256, 256, 25, 25]        --\n",
              "│    │    └─BasicConv2d: 3-11                 [256, 32, 25, 25]         (8,256)\n",
              "│    │    └─Sequential: 3-12                  [256, 32, 25, 25]         (17,536)\n",
              "│    │    └─Sequential: 3-13                  [256, 32, 25, 25]         (26,816)\n",
              "│    │    └─Conv2d: 3-14                      [256, 256, 25, 25]        (24,832)\n",
              "│    │    └─ReLU: 3-15                        [256, 256, 25, 25]        --\n",
              "│    └─Block35: 2-22                          [256, 256, 25, 25]        --\n",
              "│    │    └─BasicConv2d: 3-16                 [256, 32, 25, 25]         (8,256)\n",
              "│    │    └─Sequential: 3-17                  [256, 32, 25, 25]         (17,536)\n",
              "│    │    └─Sequential: 3-18                  [256, 32, 25, 25]         (26,816)\n",
              "│    │    └─Conv2d: 3-19                      [256, 256, 25, 25]        (24,832)\n",
              "│    │    └─ReLU: 3-20                        [256, 256, 25, 25]        --\n",
              "│    └─Block35: 2-23                          [256, 256, 25, 25]        --\n",
              "│    │    └─BasicConv2d: 3-21                 [256, 32, 25, 25]         (8,256)\n",
              "│    │    └─Sequential: 3-22                  [256, 32, 25, 25]         (17,536)\n",
              "│    │    └─Sequential: 3-23                  [256, 32, 25, 25]         (26,816)\n",
              "│    │    └─Conv2d: 3-24                      [256, 256, 25, 25]        (24,832)\n",
              "│    │    └─ReLU: 3-25                        [256, 256, 25, 25]        --\n",
              "├─Mixed_6a: 1-9                               [256, 896, 12, 12]        --\n",
              "│    └─BasicConv2d: 2-24                      [256, 384, 12, 12]        --\n",
              "│    │    └─Conv2d: 3-26                      [256, 384, 12, 12]        (884,736)\n",
              "│    │    └─BatchNorm2d: 3-27                 [256, 384, 12, 12]        (768)\n",
              "│    │    └─ReLU: 3-28                        [256, 384, 12, 12]        --\n",
              "│    └─Sequential: 2-25                       [256, 256, 12, 12]        --\n",
              "│    │    └─BasicConv2d: 3-29                 [256, 192, 25, 25]        (49,536)\n",
              "│    │    └─BasicConv2d: 3-30                 [256, 192, 25, 25]        (332,160)\n",
              "│    │    └─BasicConv2d: 3-31                 [256, 256, 12, 12]        (442,880)\n",
              "│    └─MaxPool2d: 2-26                        [256, 256, 12, 12]        --\n",
              "├─Sequential: 1-10                            [256, 896, 12, 12]        --\n",
              "│    └─Block17: 2-27                          [256, 896, 12, 12]        --\n",
              "│    │    └─BasicConv2d: 3-32                 [256, 128, 12, 12]        (114,944)\n",
              "│    │    └─Sequential: 3-33                  [256, 128, 12, 12]        (344,832)\n",
              "│    │    └─Conv2d: 3-34                      [256, 896, 12, 12]        (230,272)\n",
              "│    │    └─ReLU: 3-35                        [256, 896, 12, 12]        --\n",
              "│    └─Block17: 2-28                          [256, 896, 12, 12]        --\n",
              "│    │    └─BasicConv2d: 3-36                 [256, 128, 12, 12]        (114,944)\n",
              "│    │    └─Sequential: 3-37                  [256, 128, 12, 12]        (344,832)\n",
              "│    │    └─Conv2d: 3-38                      [256, 896, 12, 12]        (230,272)\n",
              "│    │    └─ReLU: 3-39                        [256, 896, 12, 12]        --\n",
              "│    └─Block17: 2-29                          [256, 896, 12, 12]        --\n",
              "│    │    └─BasicConv2d: 3-40                 [256, 128, 12, 12]        (114,944)\n",
              "│    │    └─Sequential: 3-41                  [256, 128, 12, 12]        (344,832)\n",
              "│    │    └─Conv2d: 3-42                      [256, 896, 12, 12]        (230,272)\n",
              "│    │    └─ReLU: 3-43                        [256, 896, 12, 12]        --\n",
              "│    └─Block17: 2-30                          [256, 896, 12, 12]        --\n",
              "│    │    └─BasicConv2d: 3-44                 [256, 128, 12, 12]        (114,944)\n",
              "│    │    └─Sequential: 3-45                  [256, 128, 12, 12]        (344,832)\n",
              "│    │    └─Conv2d: 3-46                      [256, 896, 12, 12]        (230,272)\n",
              "│    │    └─ReLU: 3-47                        [256, 896, 12, 12]        --\n",
              "│    └─Block17: 2-31                          [256, 896, 12, 12]        --\n",
              "│    │    └─BasicConv2d: 3-48                 [256, 128, 12, 12]        (114,944)\n",
              "│    │    └─Sequential: 3-49                  [256, 128, 12, 12]        (344,832)\n",
              "│    │    └─Conv2d: 3-50                      [256, 896, 12, 12]        (230,272)\n",
              "│    │    └─ReLU: 3-51                        [256, 896, 12, 12]        --\n",
              "│    └─Block17: 2-32                          [256, 896, 12, 12]        --\n",
              "│    │    └─BasicConv2d: 3-52                 [256, 128, 12, 12]        (114,944)\n",
              "│    │    └─Sequential: 3-53                  [256, 128, 12, 12]        (344,832)\n",
              "│    │    └─Conv2d: 3-54                      [256, 896, 12, 12]        (230,272)\n",
              "│    │    └─ReLU: 3-55                        [256, 896, 12, 12]        --\n",
              "│    └─Block17: 2-33                          [256, 896, 12, 12]        --\n",
              "│    │    └─BasicConv2d: 3-56                 [256, 128, 12, 12]        (114,944)\n",
              "│    │    └─Sequential: 3-57                  [256, 128, 12, 12]        (344,832)\n",
              "│    │    └─Conv2d: 3-58                      [256, 896, 12, 12]        (230,272)\n",
              "│    │    └─ReLU: 3-59                        [256, 896, 12, 12]        --\n",
              "│    └─Block17: 2-34                          [256, 896, 12, 12]        --\n",
              "│    │    └─BasicConv2d: 3-60                 [256, 128, 12, 12]        (114,944)\n",
              "│    │    └─Sequential: 3-61                  [256, 128, 12, 12]        (344,832)\n",
              "│    │    └─Conv2d: 3-62                      [256, 896, 12, 12]        (230,272)\n",
              "│    │    └─ReLU: 3-63                        [256, 896, 12, 12]        --\n",
              "│    └─Block17: 2-35                          [256, 896, 12, 12]        --\n",
              "│    │    └─BasicConv2d: 3-64                 [256, 128, 12, 12]        (114,944)\n",
              "│    │    └─Sequential: 3-65                  [256, 128, 12, 12]        (344,832)\n",
              "│    │    └─Conv2d: 3-66                      [256, 896, 12, 12]        (230,272)\n",
              "│    │    └─ReLU: 3-67                        [256, 896, 12, 12]        --\n",
              "│    └─Block17: 2-36                          [256, 896, 12, 12]        --\n",
              "│    │    └─BasicConv2d: 3-68                 [256, 128, 12, 12]        (114,944)\n",
              "│    │    └─Sequential: 3-69                  [256, 128, 12, 12]        (344,832)\n",
              "│    │    └─Conv2d: 3-70                      [256, 896, 12, 12]        (230,272)\n",
              "│    │    └─ReLU: 3-71                        [256, 896, 12, 12]        --\n",
              "├─Mixed_7a: 1-11                              [256, 1792, 5, 5]         --\n",
              "│    └─Sequential: 2-37                       [256, 384, 5, 5]          --\n",
              "│    │    └─BasicConv2d: 3-72                 [256, 256, 12, 12]        (229,888)\n",
              "│    │    └─BasicConv2d: 3-73                 [256, 384, 5, 5]          (885,504)\n",
              "│    └─Sequential: 2-38                       [256, 256, 5, 5]          --\n",
              "│    │    └─BasicConv2d: 3-74                 [256, 256, 12, 12]        (229,888)\n",
              "│    │    └─BasicConv2d: 3-75                 [256, 256, 5, 5]          (590,336)\n",
              "│    └─Sequential: 2-39                       [256, 256, 5, 5]          --\n",
              "│    │    └─BasicConv2d: 3-76                 [256, 256, 12, 12]        (229,888)\n",
              "│    │    └─BasicConv2d: 3-77                 [256, 256, 12, 12]        (590,336)\n",
              "│    │    └─BasicConv2d: 3-78                 [256, 256, 5, 5]          (590,336)\n",
              "│    └─MaxPool2d: 2-40                        [256, 896, 5, 5]          --\n",
              "├─Sequential: 1-12                            [256, 1792, 5, 5]         --\n",
              "│    └─Block8: 2-41                           [256, 1792, 5, 5]         --\n",
              "│    │    └─BasicConv2d: 3-79                 [256, 192, 5, 5]          (344,448)\n",
              "│    │    └─Sequential: 3-80                  [256, 192, 5, 5]          (566,400)\n",
              "│    │    └─Conv2d: 3-81                      [256, 1792, 5, 5]         (689,920)\n",
              "│    │    └─ReLU: 3-82                        [256, 1792, 5, 5]         --\n",
              "│    └─Block8: 2-42                           [256, 1792, 5, 5]         --\n",
              "│    │    └─BasicConv2d: 3-83                 [256, 192, 5, 5]          (344,448)\n",
              "│    │    └─Sequential: 3-84                  [256, 192, 5, 5]          (566,400)\n",
              "│    │    └─Conv2d: 3-85                      [256, 1792, 5, 5]         (689,920)\n",
              "│    │    └─ReLU: 3-86                        [256, 1792, 5, 5]         --\n",
              "│    └─Block8: 2-43                           [256, 1792, 5, 5]         --\n",
              "│    │    └─BasicConv2d: 3-87                 [256, 192, 5, 5]          (344,448)\n",
              "│    │    └─Sequential: 3-88                  [256, 192, 5, 5]          (566,400)\n",
              "│    │    └─Conv2d: 3-89                      [256, 1792, 5, 5]         (689,920)\n",
              "│    │    └─ReLU: 3-90                        [256, 1792, 5, 5]         --\n",
              "│    └─Block8: 2-44                           [256, 1792, 5, 5]         --\n",
              "│    │    └─BasicConv2d: 3-91                 [256, 192, 5, 5]          (344,448)\n",
              "│    │    └─Sequential: 3-92                  [256, 192, 5, 5]          (566,400)\n",
              "│    │    └─Conv2d: 3-93                      [256, 1792, 5, 5]         (689,920)\n",
              "│    │    └─ReLU: 3-94                        [256, 1792, 5, 5]         --\n",
              "│    └─Block8: 2-45                           [256, 1792, 5, 5]         --\n",
              "│    │    └─BasicConv2d: 3-95                 [256, 192, 5, 5]          (344,448)\n",
              "│    │    └─Sequential: 3-96                  [256, 192, 5, 5]          (566,400)\n",
              "│    │    └─Conv2d: 3-97                      [256, 1792, 5, 5]         (689,920)\n",
              "│    │    └─ReLU: 3-98                        [256, 1792, 5, 5]         --\n",
              "├─Block8: 1-13                                [256, 1792, 5, 5]         --\n",
              "│    └─BasicConv2d: 2-46                      [256, 192, 5, 5]          --\n",
              "│    │    └─Conv2d: 3-99                      [256, 192, 5, 5]          344,064\n",
              "│    │    └─BatchNorm2d: 3-100                [256, 192, 5, 5]          384\n",
              "│    │    └─ReLU: 3-101                       [256, 192, 5, 5]          --\n",
              "│    └─Sequential: 2-47                       [256, 192, 5, 5]          --\n",
              "│    │    └─BasicConv2d: 3-102                [256, 192, 5, 5]          344,448\n",
              "│    │    └─BasicConv2d: 3-103                [256, 192, 5, 5]          110,976\n",
              "│    │    └─BasicConv2d: 3-104                [256, 192, 5, 5]          110,976\n",
              "│    └─Conv2d: 2-48                           [256, 1792, 5, 5]         689,920\n",
              "├─AdaptiveAvgPool2d: 1-14                     [256, 1792, 1, 1]         --\n",
              "├─Dropout: 1-15                               [256, 1792, 1, 1]         --\n",
              "├─Linear: 1-16                                [256, 512]                917,504\n",
              "├─BatchNorm1d: 1-17                           [256, 512]                1,024\n",
              "├─Linear: 1-18                                [256, 1]                  513\n",
              "===============================================================================================\n",
              "Total params: 23,483,137\n",
              "Trainable params: 2,519,809\n",
              "Non-trainable params: 20,963,328\n",
              "Total mult-adds (G): 804.60\n",
              "===============================================================================================\n",
              "Input size (MB): 154.14\n",
              "Forward/backward pass size (MB): 22863.15\n",
              "Params size (MB): 93.93\n",
              "Estimated Total Size (MB): 23111.23\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCj-n1ApThX7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a715fe4-8e17-485b-9e68-bae13050b1fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 : [256 / 40882] loss : 22.37364959716797, Batch time: 0.7102 sec\n",
            "Epoch 1 : [2816 / 40882] loss : 22.72926902770996, Batch time: 0.8148 sec\n",
            "Epoch 1 : [5376 / 40882] loss : 23.503276824951172, Batch time: 0.7009 sec\n",
            "Epoch 1 : [7936 / 40882] loss : 21.73055076599121, Batch time: 0.7593 sec\n",
            "Epoch 1 : [10496 / 40882] loss : 23.04936408996582, Batch time: 0.7216 sec\n",
            "Epoch 1 : [13056 / 40882] loss : 23.541423797607422, Batch time: 0.7979 sec\n",
            "Epoch 1 : [15616 / 40882] loss : 22.43260955810547, Batch time: 0.7020 sec\n",
            "Epoch 1 : [18176 / 40882] loss : 22.472015380859375, Batch time: 0.7178 sec\n",
            "Epoch 1 : [20736 / 40882] loss : 22.748119354248047, Batch time: 0.7874 sec\n",
            "Epoch 1 : [23296 / 40882] loss : 22.626632690429688, Batch time: 0.7777 sec\n",
            "Epoch 1 : [25856 / 40882] loss : 23.942699432373047, Batch time: 0.7164 sec\n",
            "Epoch 1 : [28416 / 40882] loss : 20.647220611572266, Batch time: 0.7717 sec\n",
            "Epoch 1 : [30976 / 40882] loss : 19.21630096435547, Batch time: 0.7567 sec\n",
            "Epoch 1 : [33536 / 40882] loss : 20.398740768432617, Batch time: 0.7535 sec\n",
            "Epoch 1 : [36096 / 40882] loss : 20.61562728881836, Batch time: 0.7028 sec\n",
            "Epoch 1 : [38656 / 40882] loss : 21.4740047454834, Batch time: 0.7958 sec\n",
            "Epoch 1 Finished, Total Epoch time: 6846.82908 sec, Train Loss: 22.03221\n",
            "Epoch : 1, Loss : 22.03221, Val_loss : 20.58909\n",
            "Epoch 2 : [256 / 40882] loss : 19.087688446044922, Batch time: 0.7005 sec\n",
            "Epoch 2 : [2816 / 40882] loss : 19.86306381225586, Batch time: 0.7013 sec\n",
            "Epoch 2 : [5376 / 40882] loss : 19.075891494750977, Batch time: 0.7267 sec\n",
            "Epoch 2 : [7936 / 40882] loss : 19.46210479736328, Batch time: 0.7438 sec\n",
            "Epoch 2 : [10496 / 40882] loss : 19.040815353393555, Batch time: 0.7909 sec\n",
            "Epoch 2 : [13056 / 40882] loss : 20.939960479736328, Batch time: 0.7358 sec\n",
            "Epoch 2 : [15616 / 40882] loss : 20.026029586791992, Batch time: 0.7538 sec\n",
            "Epoch 2 : [18176 / 40882] loss : 16.701494216918945, Batch time: 0.7249 sec\n",
            "Epoch 2 : [20736 / 40882] loss : 17.838104248046875, Batch time: 0.7136 sec\n",
            "Epoch 2 : [23296 / 40882] loss : 17.41067123413086, Batch time: 0.7674 sec\n",
            "Epoch 2 : [25856 / 40882] loss : 17.278366088867188, Batch time: 0.6921 sec\n",
            "Epoch 2 : [28416 / 40882] loss : 17.331523895263672, Batch time: 0.6750 sec\n",
            "Epoch 2 : [30976 / 40882] loss : 14.757843017578125, Batch time: 0.7733 sec\n",
            "Epoch 2 : [33536 / 40882] loss : 16.01717758178711, Batch time: 0.7391 sec\n",
            "Epoch 2 : [36096 / 40882] loss : 13.45564079284668, Batch time: 0.7034 sec\n",
            "Epoch 2 : [38656 / 40882] loss : 11.82048225402832, Batch time: 0.6988 sec\n",
            "Epoch 2 Finished, Total Epoch time: 7122.34062 sec, Train Loss: 17.07232\n",
            "Epoch : 2, Loss : 17.07232, Val_loss : 11.06361\n",
            "Epoch 3 : [256 / 40882] loss : 12.55392837524414, Batch time: 0.8171 sec\n",
            "Epoch 3 : [2816 / 40882] loss : 14.672454833984375, Batch time: 0.6956 sec\n",
            "Epoch 3 : [5376 / 40882] loss : 9.78669548034668, Batch time: 0.6987 sec\n",
            "Epoch 3 : [7936 / 40882] loss : 10.843740463256836, Batch time: 0.7634 sec\n",
            "Epoch 3 : [10496 / 40882] loss : 9.579066276550293, Batch time: 0.8125 sec\n",
            "Epoch 3 : [13056 / 40882] loss : 8.556241989135742, Batch time: 0.6913 sec\n",
            "Epoch 3 : [15616 / 40882] loss : 8.36701774597168, Batch time: 0.7089 sec\n",
            "Epoch 3 : [18176 / 40882] loss : 7.150254726409912, Batch time: 0.7655 sec\n",
            "Epoch 3 : [20736 / 40882] loss : 8.465704917907715, Batch time: 0.7342 sec\n",
            "Epoch 3 : [23296 / 40882] loss : 7.271046161651611, Batch time: 0.6983 sec\n",
            "Epoch 3 : [25856 / 40882] loss : 6.944917678833008, Batch time: 0.7376 sec\n",
            "Epoch 3 : [28416 / 40882] loss : 7.437872409820557, Batch time: 0.6866 sec\n",
            "Epoch 3 : [30976 / 40882] loss : 6.592169761657715, Batch time: 0.6872 sec\n",
            "Epoch 3 : [33536 / 40882] loss : 7.2469024658203125, Batch time: 0.7164 sec\n",
            "Epoch 3 : [36096 / 40882] loss : 5.634432315826416, Batch time: 0.7138 sec\n",
            "Epoch 3 : [38656 / 40882] loss : 6.36650276184082, Batch time: 0.7340 sec\n",
            "Epoch 3 Finished, Total Epoch time: 6736.41318 sec, Train Loss: 8.42461\n",
            "Epoch : 3, Loss : 8.42461, Val_loss : 5.66821\n",
            "Epoch 4 : [256 / 40882] loss : 5.5771307945251465, Batch time: 0.7728 sec\n",
            "Epoch 4 : [2816 / 40882] loss : 4.943718910217285, Batch time: 0.7725 sec\n",
            "Epoch 4 : [5376 / 40882] loss : 5.618508338928223, Batch time: 0.7785 sec\n",
            "Epoch 4 : [7936 / 40882] loss : 5.797525405883789, Batch time: 0.7773 sec\n",
            "Epoch 4 : [10496 / 40882] loss : 4.9310832023620605, Batch time: 0.7018 sec\n",
            "Epoch 4 : [13056 / 40882] loss : 5.034939765930176, Batch time: 0.7656 sec\n",
            "Epoch 4 : [15616 / 40882] loss : 5.188817024230957, Batch time: 0.7710 sec\n",
            "Epoch 4 : [18176 / 40882] loss : 4.940755844116211, Batch time: 0.6901 sec\n",
            "Epoch 4 : [20736 / 40882] loss : 4.519649028778076, Batch time: 0.6886 sec\n",
            "Epoch 4 : [23296 / 40882] loss : 4.872992515563965, Batch time: 0.7149 sec\n",
            "Epoch 4 : [25856 / 40882] loss : 4.806879043579102, Batch time: 0.6974 sec\n",
            "Epoch 4 : [28416 / 40882] loss : 4.640474319458008, Batch time: 0.7680 sec\n",
            "Epoch 4 : [30976 / 40882] loss : 4.662914276123047, Batch time: 0.7051 sec\n",
            "Epoch 4 : [33536 / 40882] loss : 4.926239967346191, Batch time: 0.7767 sec\n",
            "Epoch 4 : [36096 / 40882] loss : 4.967575550079346, Batch time: 0.7139 sec\n",
            "Epoch 4 : [38656 / 40882] loss : 5.45658016204834, Batch time: 0.7350 sec\n",
            "Epoch 4 Finished, Total Epoch time: 6691.91570 sec, Train Loss: 5.18108\n",
            "Epoch : 4, Loss : 5.18108, Val_loss : 4.88074\n",
            "Epoch 5 : [256 / 40882] loss : 4.467127323150635, Batch time: 0.7275 sec\n",
            "Epoch 5 : [2816 / 40882] loss : 4.714517593383789, Batch time: 0.7376 sec\n",
            "Epoch 5 : [5376 / 40882] loss : 4.986167907714844, Batch time: 0.7191 sec\n",
            "Epoch 5 : [7936 / 40882] loss : 4.682889938354492, Batch time: 0.7552 sec\n",
            "Epoch 5 : [10496 / 40882] loss : 4.585518836975098, Batch time: 0.7173 sec\n",
            "Epoch 5 : [13056 / 40882] loss : 3.9987635612487793, Batch time: 0.7857 sec\n",
            "Epoch 5 : [15616 / 40882] loss : 4.917614936828613, Batch time: 0.7699 sec\n",
            "Epoch 5 : [18176 / 40882] loss : 3.9297120571136475, Batch time: 0.7221 sec\n",
            "Epoch 5 : [20736 / 40882] loss : 4.43144416809082, Batch time: 0.7163 sec\n",
            "Epoch 5 : [23296 / 40882] loss : 4.684196472167969, Batch time: 0.7354 sec\n",
            "Epoch 5 : [25856 / 40882] loss : 4.653419494628906, Batch time: 0.8163 sec\n",
            "Epoch 5 : [28416 / 40882] loss : 4.48905086517334, Batch time: 0.7267 sec\n",
            "Epoch 5 : [30976 / 40882] loss : 4.6366071701049805, Batch time: 0.7635 sec\n",
            "Epoch 5 : [33536 / 40882] loss : 4.161641597747803, Batch time: 0.7933 sec\n",
            "Epoch 5 : [36096 / 40882] loss : 5.190190315246582, Batch time: 0.7906 sec\n",
            "Epoch 5 : [38656 / 40882] loss : 4.691429138183594, Batch time: 0.7250 sec\n",
            "Epoch 5 Finished, Total Epoch time: 6548.45039 sec, Train Loss: 4.75659\n",
            "Epoch : 5, Loss : 4.75659, Val_loss : 4.87239\n",
            "Epoch 6 : [256 / 40882] loss : 5.160335540771484, Batch time: 0.7345 sec\n",
            "Epoch 6 : [2816 / 40882] loss : 4.271983623504639, Batch time: 0.7451 sec\n",
            "Epoch 6 : [5376 / 40882] loss : 4.495651721954346, Batch time: 0.8209 sec\n",
            "Epoch 6 : [7936 / 40882] loss : 5.040729522705078, Batch time: 0.7536 sec\n",
            "Epoch 6 : [10496 / 40882] loss : 4.30490255355835, Batch time: 0.7261 sec\n",
            "Epoch 6 : [13056 / 40882] loss : 4.699731349945068, Batch time: 0.8372 sec\n",
            "Epoch 6 : [15616 / 40882] loss : 4.45504093170166, Batch time: 0.6954 sec\n",
            "Epoch 6 : [18176 / 40882] loss : 4.257437229156494, Batch time: 0.7239 sec\n",
            "Epoch 6 : [20736 / 40882] loss : 4.734827518463135, Batch time: 0.8059 sec\n",
            "Epoch 6 : [23296 / 40882] loss : 4.485689640045166, Batch time: 0.7083 sec\n",
            "Epoch 6 : [25856 / 40882] loss : 4.489020824432373, Batch time: 0.7421 sec\n",
            "Epoch 6 : [28416 / 40882] loss : 4.593073844909668, Batch time: 0.7576 sec\n",
            "Epoch 6 : [30976 / 40882] loss : 4.308218479156494, Batch time: 0.7316 sec\n",
            "Epoch 6 : [33536 / 40882] loss : 4.940187931060791, Batch time: 0.7968 sec\n",
            "Epoch 6 : [36096 / 40882] loss : 4.415544509887695, Batch time: 0.7339 sec\n",
            "Epoch 6 : [38656 / 40882] loss : 4.764034271240234, Batch time: 0.7178 sec\n",
            "Epoch 6 Finished, Total Epoch time: 6451.11431 sec, Train Loss: 4.54383\n",
            "Epoch : 6, Loss : 4.54383, Val_loss : 4.71759\n",
            "Epoch 7 : [256 / 40882] loss : 4.238450527191162, Batch time: 0.7520 sec\n",
            "Epoch 7 : [2816 / 40882] loss : 4.287569999694824, Batch time: 0.7011 sec\n",
            "Epoch 7 : [5376 / 40882] loss : 4.2646403312683105, Batch time: 0.7361 sec\n",
            "Epoch 7 : [7936 / 40882] loss : 4.0374298095703125, Batch time: 0.7824 sec\n",
            "Epoch 7 : [10496 / 40882] loss : 4.612855911254883, Batch time: 0.7142 sec\n",
            "Epoch 7 : [13056 / 40882] loss : 4.637015342712402, Batch time: 0.7102 sec\n",
            "Epoch 7 : [15616 / 40882] loss : 5.079678535461426, Batch time: 0.8248 sec\n",
            "Epoch 7 : [18176 / 40882] loss : 4.3501482009887695, Batch time: 0.7701 sec\n",
            "Epoch 7 : [20736 / 40882] loss : 4.536599159240723, Batch time: 0.7149 sec\n",
            "Epoch 7 : [23296 / 40882] loss : 4.482282638549805, Batch time: 0.7629 sec\n",
            "Epoch 7 : [25856 / 40882] loss : 3.8391189575195312, Batch time: 0.7101 sec\n",
            "Epoch 7 : [28416 / 40882] loss : 4.302341461181641, Batch time: 0.7965 sec\n",
            "Epoch 7 : [30976 / 40882] loss : 4.4350056648254395, Batch time: 0.7114 sec\n",
            "Epoch 7 : [33536 / 40882] loss : 4.716977119445801, Batch time: 0.7756 sec\n",
            "Epoch 7 : [36096 / 40882] loss : 4.389444828033447, Batch time: 0.7611 sec\n",
            "Epoch 7 : [38656 / 40882] loss : 4.404111862182617, Batch time: 0.7980 sec\n",
            "Epoch 7 Finished, Total Epoch time: 6492.24927 sec, Train Loss: 4.37443\n",
            "Epoch : 7, Loss : 4.37443, Val_loss : 4.71527\n",
            "Epoch 8 : [256 / 40882] loss : 4.352551460266113, Batch time: 0.7295 sec\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-7706615e4706>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mage_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop_age\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_age\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_loop_age\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mage_lr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-3600e2f18911>\u001b[0m in \u001b[0;36mtrain_loop_age\u001b[0;34m(dataloader, model, loss_fn, optimizer, epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# start = time.time()\n",
        "# for epoch in range(150):\n",
        "#     age_loss = train_loop_age(train_loader, age_model, nn.L1Loss(), opt_age, epoch)\n",
        "#     val_loss = validation_loop_age(val_loader, age_model, nn.L1Loss())\n",
        "#     age_lr_scheduler.step(val_loss)\n",
        "\n",
        "#     early_stopping(val_loss)\n",
        "#     if early_stopping.early_stop:\n",
        "#         print(\"Early stopping\")\n",
        "#         save_model(epoch, age_model, opt_age, '/content/drive/MyDrive/DL_DATA/DL_Face_REC/Model/inception_resnet_v1/age_model_checkpoint.pth', age_loss, val_loss, age_lr_scheduler)\n",
        "#         break\n",
        "\n",
        "#     save_model(epoch, age_model, opt_age,\n",
        "#                f'/content/drive/MyDrive/DL_DATA/DL_Face_REC/Model/inception_resnet_v1/age_model_checkpoint_epoch_{epoch+1}_loss_{round(val_loss, 2)}.pth',\n",
        "#                age_loss, val_loss, age_lr_scheduler)\n",
        "#     print(f'Epoch : {epoch+1}, Loss : {age_loss:.5f}, Val_loss : {val_loss:.5f}')\n",
        "\n",
        "# total_time = time.time() - start\n",
        "\n",
        "# # 전체 학습 시간 출력\n",
        "# hours, rem = divmod(total_time, 3600)\n",
        "# minutes, seconds = divmod(rem, 60)\n",
        "# print(\"Total training time: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 이어서 학습"
      ],
      "metadata": {
        "id": "rdTocioWSQht"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHEuSHa4nG19"
      },
      "outputs": [],
      "source": [
        "# age 모델 이어서 학습\n",
        "def load_model(model, optimizer, path, scheduler):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "    return model, optimizer, epoch, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델과 옵티마이저, 스케줄러\n",
        "age_model = make_age_model().to(device)\n",
        "opt_age = optim.Adam(age_model.parameters(), lr=0.0003)\n",
        "age_lr_scheduler = ReduceLROnPlateau(opt_age, mode='min', verbose=True, cooldown=3)\n",
        "\n",
        "# 체크포인트 불러오기\n",
        "age_model, opt_age, start_epoch, age_lr_scheduler = load_model(age_model, opt_age,\n",
        "                                             '/content/drive/MyDrive/DL_DATA/DL_Face_REC/Model/inception_resnet_v1/age_model_checkpoint_epoch_7_loss_4.72.pth',\n",
        "                                                               age_lr_scheduler)\n",
        "\n",
        "# 학습 재개\n",
        "start = time.time()\n",
        "for epoch in range(start_epoch + 1, 150):\n",
        "    age_loss = train_loop_age(train_loader, age_model, nn.L1Loss(), opt_age, epoch)\n",
        "    val_loss = validation_loop_age(val_loader, age_model, nn.L1Loss())\n",
        "    age_lr_scheduler.step(val_loss)\n",
        "\n",
        "    early_stopping(val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        save_model(epoch, age_model, opt_age, '/content/drive/MyDrive/DL_DATA/DL_Face_REC/Model/inception_resnet_v1/age_model_checkpoint.pth',\n",
        "                   age_loss, val_loss, age_lr_scheduler)\n",
        "        break\n",
        "\n",
        "    save_model(epoch, age_model, opt_age,\n",
        "               f'/content/drive/MyDrive/DL_DATA/DL_Face_REC/Model/inception_resnet_v1/age_model_checkpoint_epoch_{epoch+1}_loss_{round(val_loss, 2)}.pth',\n",
        "               age_loss, val_loss, age_lr_scheduler)\n",
        "\n",
        "    print(f'Epoch : {epoch + 1}, Loss : {age_loss}, Val_loss : {val_loss}')\n",
        "\n",
        "total_time = time.time() - start\n",
        "\n",
        "hours, rem = divmod(total_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(\"Total training time: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8ek1ujyDQZaQ",
        "outputId": "a148f4bb-f7ec-4af5-b404-d50d38e62b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 : [256 / 40882] loss : 4.3025126457214355, Batch time: 0.7389 sec\n",
            "Epoch 8 : [2816 / 40882] loss : 4.3405537605285645, Batch time: 0.7133 sec\n",
            "Epoch 8 : [5376 / 40882] loss : 4.614660263061523, Batch time: 0.6860 sec\n",
            "Epoch 8 : [7936 / 40882] loss : 5.0754570960998535, Batch time: 0.7100 sec\n",
            "Epoch 8 : [10496 / 40882] loss : 3.879998207092285, Batch time: 0.7308 sec\n",
            "Epoch 8 : [13056 / 40882] loss : 4.335206031799316, Batch time: 0.6968 sec\n",
            "Epoch 8 : [15616 / 40882] loss : 4.504642486572266, Batch time: 0.7812 sec\n",
            "Epoch 8 : [18176 / 40882] loss : 3.922011375427246, Batch time: 0.7267 sec\n",
            "Epoch 8 : [20736 / 40882] loss : 4.035374641418457, Batch time: 0.7830 sec\n",
            "Epoch 8 : [23296 / 40882] loss : 4.30877685546875, Batch time: 0.7207 sec\n",
            "Epoch 8 : [25856 / 40882] loss : 3.931520700454712, Batch time: 0.7423 sec\n",
            "Epoch 8 : [28416 / 40882] loss : 3.8826920986175537, Batch time: 0.7767 sec\n",
            "Epoch 8 : [30976 / 40882] loss : 4.561410903930664, Batch time: 0.7084 sec\n",
            "Epoch 8 : [33536 / 40882] loss : 4.220831871032715, Batch time: 0.7009 sec\n",
            "Epoch 8 : [36096 / 40882] loss : 3.8763325214385986, Batch time: 0.7278 sec\n",
            "Epoch 8 : [38656 / 40882] loss : 4.847139358520508, Batch time: 0.8068 sec\n",
            "Epoch 8 Finished, Total Epoch time: 6528.44819 sec, Train Loss: 4.24516\n",
            "Epoch : 8, Loss : 4.245163451135158, Val_loss : 4.642987310886383\n",
            "Epoch 9 : [256 / 40882] loss : 3.8112149238586426, Batch time: 0.7518 sec\n",
            "Epoch 9 : [2816 / 40882] loss : 4.563185691833496, Batch time: 0.7514 sec\n",
            "Epoch 9 : [5376 / 40882] loss : 4.819208145141602, Batch time: 0.7698 sec\n",
            "Epoch 9 : [7936 / 40882] loss : 4.196592330932617, Batch time: 0.6901 sec\n",
            "Epoch 9 : [10496 / 40882] loss : 4.231956481933594, Batch time: 0.7130 sec\n",
            "Epoch 9 : [13056 / 40882] loss : 4.806614875793457, Batch time: 0.7194 sec\n",
            "Epoch 9 : [15616 / 40882] loss : 4.241837501525879, Batch time: 0.7182 sec\n",
            "Epoch 9 : [18176 / 40882] loss : 4.393407821655273, Batch time: 0.6869 sec\n",
            "Epoch 9 : [20736 / 40882] loss : 3.8308255672454834, Batch time: 0.7050 sec\n",
            "Epoch 9 : [23296 / 40882] loss : 3.821280002593994, Batch time: 0.7110 sec\n",
            "Epoch 9 : [25856 / 40882] loss : 4.938172817230225, Batch time: 0.6988 sec\n",
            "Epoch 9 : [28416 / 40882] loss : 4.832233428955078, Batch time: 0.7184 sec\n",
            "Epoch 9 : [30976 / 40882] loss : 4.795092582702637, Batch time: 0.6971 sec\n",
            "Epoch 9 : [33536 / 40882] loss : 3.5819270610809326, Batch time: 0.7667 sec\n",
            "Epoch 9 : [36096 / 40882] loss : 4.180006980895996, Batch time: 0.6978 sec\n",
            "Epoch 9 : [38656 / 40882] loss : 4.137653350830078, Batch time: 0.7261 sec\n",
            "Epoch 9 Finished, Total Epoch time: 6601.56298 sec, Train Loss: 4.14693\n",
            "Epoch : 9, Loss : 4.14693269431591, Val_loss : 4.670093894004822\n",
            "Epoch 10 : [256 / 40882] loss : 4.613121032714844, Batch time: 0.8060 sec\n",
            "Epoch 10 : [2816 / 40882] loss : 4.262589454650879, Batch time: 0.7293 sec\n",
            "Epoch 10 : [5376 / 40882] loss : 3.7600531578063965, Batch time: 0.8174 sec\n",
            "Epoch 10 : [7936 / 40882] loss : 4.139690399169922, Batch time: 0.7973 sec\n",
            "Epoch 10 : [10496 / 40882] loss : 3.643848419189453, Batch time: 0.7390 sec\n",
            "Epoch 10 : [13056 / 40882] loss : 4.5378804206848145, Batch time: 0.7743 sec\n",
            "Epoch 10 : [15616 / 40882] loss : 3.8928306102752686, Batch time: 0.6956 sec\n",
            "Epoch 10 : [18176 / 40882] loss : 4.022762298583984, Batch time: 0.7555 sec\n",
            "Epoch 10 : [20736 / 40882] loss : 4.01981782913208, Batch time: 0.7505 sec\n",
            "Epoch 10 : [23296 / 40882] loss : 4.501163482666016, Batch time: 0.7819 sec\n",
            "Epoch 10 : [25856 / 40882] loss : 4.578014373779297, Batch time: 0.8002 sec\n",
            "Epoch 10 : [28416 / 40882] loss : 3.8383994102478027, Batch time: 0.7985 sec\n",
            "Epoch 10 : [30976 / 40882] loss : 4.165226936340332, Batch time: 0.7017 sec\n",
            "Epoch 10 : [33536 / 40882] loss : 4.081562519073486, Batch time: 0.7866 sec\n",
            "Epoch 10 : [36096 / 40882] loss : 4.318796157836914, Batch time: 0.7159 sec\n",
            "Epoch 10 : [38656 / 40882] loss : 4.548486709594727, Batch time: 0.7182 sec\n",
            "Epoch 10 Finished, Total Epoch time: 6555.74046 sec, Train Loss: 4.08069\n",
            "Epoch : 10, Loss : 4.080686891078949, Val_loss : 4.573235726356506\n",
            "Epoch 11 : [256 / 40882] loss : 3.882965326309204, Batch time: 0.7281 sec\n",
            "Epoch 11 : [2816 / 40882] loss : 3.234048843383789, Batch time: 0.7012 sec\n",
            "Epoch 11 : [5376 / 40882] loss : 4.425613880157471, Batch time: 0.7625 sec\n",
            "Epoch 11 : [7936 / 40882] loss : 4.126113414764404, Batch time: 0.8226 sec\n",
            "Epoch 11 : [10496 / 40882] loss : 4.979999542236328, Batch time: 0.7574 sec\n",
            "Epoch 11 : [13056 / 40882] loss : 4.567862033843994, Batch time: 0.7378 sec\n",
            "Epoch 11 : [15616 / 40882] loss : 3.5476737022399902, Batch time: 0.7218 sec\n",
            "Epoch 11 : [18176 / 40882] loss : 3.5569114685058594, Batch time: 0.7687 sec\n",
            "Epoch 11 : [20736 / 40882] loss : 3.6958346366882324, Batch time: 0.7102 sec\n",
            "Epoch 11 : [23296 / 40882] loss : 3.847874641418457, Batch time: 0.7312 sec\n",
            "Epoch 11 : [25856 / 40882] loss : 3.449620246887207, Batch time: 0.7184 sec\n",
            "Epoch 11 : [28416 / 40882] loss : 3.781533718109131, Batch time: 0.7690 sec\n",
            "Epoch 11 : [30976 / 40882] loss : 3.9762203693389893, Batch time: 0.8074 sec\n",
            "Epoch 11 : [33536 / 40882] loss : 4.151557922363281, Batch time: 0.8237 sec\n",
            "Epoch 11 : [36096 / 40882] loss : 3.9991893768310547, Batch time: 0.7908 sec\n",
            "Epoch 11 : [38656 / 40882] loss : 4.099226474761963, Batch time: 0.7094 sec\n",
            "Epoch 11 Finished, Total Epoch time: 6591.66533 sec, Train Loss: 4.00695\n",
            "Epoch : 11, Loss : 4.006946535408497, Val_loss : 4.663610517978668\n",
            "Epoch 12 : [256 / 40882] loss : 3.8610568046569824, Batch time: 0.7757 sec\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-b5fd0a9b96ce>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mage_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop_age\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_age\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_loop_age\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mage_lr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-3600e2f18911>\u001b[0m in \u001b[0;36mtrain_loop_age\u001b[0;34m(dataloader, model, loss_fn, optimizer, epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 에포크 시작 시간\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 배치 처리 시작 시간\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age_past'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inception ResNet v1\n",
        "- Epoch : 1, Loss : 22.03221, Val_loss : 20.58909\n",
        "- Epoch : 2, Loss : 17.07232, Val_loss : 11.06361\n",
        "- Epoch : 3, Loss : 8.42461, Val_loss : 5.66821\n",
        "- Epoch : 4, Loss : 5.18108, Val_loss : 4.88074\n",
        "- Epoch : 5, Loss : 4.75659, Val_loss : 4.87239\n",
        "- Epoch : 6, Loss : 4.54383, Val_loss : 4.71759\n",
        "- Epoch : 7, Loss : 4.37443, Val_loss : 4.71527\n",
        "- Epoch : 8, Loss : 4.245163451135158, Val_loss : 4.642987310886383\n",
        "- Epoch : 9, Loss : 4.14693269431591, Val_loss : 4.670093894004822\n",
        "- Epoch : 10, Loss : 4.080686891078949, Val_loss : 4.573235726356506\n",
        "- Epoch : 11, Loss : 4.006946535408497, Val_loss : 4.663610517978668"
      ],
      "metadata": {
        "id": "Agfz-3fM0Jm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QXPrYNCN0JZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resnet18\n",
        "- Epoch 1 finished, Total Epoch time: 5526.9220 sec\n",
        "- Epoch : 1, Loss : 230.3899026858579, Val_loss : 181.15303230285645\n",
        "- Epoch : 2, Loss : 87.90939525434166, Val_loss : 114.64087181091308\n",
        "- Epoch : 3, Loss : 63.55927043355954, Val_loss : 76.98570957183838\n",
        "- Epoch : 4, Loss : 50.9931274219683, Val_loss : 54.968890857696536\n",
        "- Epoch : 5, Loss : 43.18443214513694, Val_loss : 50.175275039672854\n",
        "- Epoch : 6, Loss : 36.12652318189099, Val_loss : 62.178778648376465\n",
        "- Epoch : 7, Loss : 32.96918888456503, Val_loss : 45.514417791366576\n",
        "- Epoch : 8, Loss : 28.73120542392609, Val_loss : 44.439664649963376\n",
        "- Epoch : 9, Loss : 26.715184278548904, Val_loss : 44.155246591567995\n",
        "- Epoch : 10, Loss : 24.47770042176459, Val_loss : 45.13270831108093\n",
        "-> 검증데이터 손실이 감소하지 않아, colorjitter 추가\n",
        "- transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)\n",
        "- Epoch : 11, Loss : 23.683984027546682, Val_loss : 47.31354990005493\n",
        "- Epoch : 12, Loss : 21.20979366788439, Val_loss : 38.79328966140747\n",
        "- Epoch : 13, Loss : 21.470603432624962, Val_loss : 38.411310482025144\n",
        "- Epoch : 14, Loss : 19.810259369528218, Val_loss : 44.32828512191772\n",
        "- Epoch : 15, Loss : 18.40842961353861, Val_loss : 36.42188596725464\n",
        "- Epoch : 16, Loss : 17.623453067366484, Val_loss : 35.0176604270935\n",
        "- Epoch : 17, Loss : 16.51842679795186, Val_loss : 42.73808040618896\n",
        "- Epoch : 18, Loss : 16.016441867609693, Val_loss : 34.663173627853396\n",
        "- Epoch : 19, Loss : 14.968255115922089, Val_loss : 37.80206971168518\n",
        "- Epoch : 20, Loss : 14.573177337646484, Val_loss : 46.45549983978272\n",
        "- Epoch : 21, Loss : 14.00512559246865, Val_loss : 34.19767136573792\n",
        "- Epoch : 22, Loss : 11.325439486533973, Val_loss : 39.99099979400635\n",
        "-> 60대 이상에 대해 오버 샘플링(oversample_rate=4, random_apply 0.6)\n",
        "- Epoch : 22, Loss : 15.139811009168625, Val_loss : 35.96300897598267\n",
        "- Epoch : 23, Loss : 14.79718075990677, Val_loss : 33.31015124320984\n",
        "- Epoch : 24, Loss : 14.064417380094529, Val_loss : 50.18931665420532"
      ],
      "metadata": {
        "id": "shHS0Orsl6Si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# epochs = list(range(1, 25))\n",
        "# train_loss = []\n",
        "# val_loss = []\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(epochs, train_loss, label='Training Loss', color='blue')\n",
        "# plt.plot(epochs, val_loss, label='Validation Loss', color='red')\n",
        "# plt.title('Training and Validation Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "DP6zdyP5ddri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader.dataset.close()\n",
        "val_loader.dataset.close()"
      ],
      "metadata": {
        "id": "i_x6yyJj5Q0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ7hC1KBgo8H"
      },
      "source": [
        "# VGG\n",
        "- Epoch : 1, Loss : 183.76921428333628, Val_loss : 138.7342578125\n",
        "- Epoch : 2, Loss : 128.2447030203683, Val_loss : 143.2067679595947\n",
        "- Epoch : 3, Loss : 122.56000444965977, Val_loss : 96.83059211730956\n",
        "- Epoch : 4, Loss : 87.70850903081437, Val_loss : 78.38082936604818\n",
        "- Epoch : 5, Loss : 68.04693739406598, Val_loss : 72.64038126627604\n",
        "- Epoch : 6, Loss : 57.276853927027304, Val_loss : 68.96054336547851\n",
        "- Epoch : 7, Loss : 51.59112667961242, Val_loss : 57.317286516825355\n",
        "- Epoch : 8, Loss : 47.01944582150006, Val_loss : 55.66085670471191\n",
        "- Epoch : 9, Loss : 42.54086226624803, Val_loss : 53.47876875559489\n",
        "- Epoch : 10, Loss : 39.80607411198723, Val_loss : 54.47281494140625\n",
        "- Epoch : 11, Loss : 36.430880546569824, Val_loss : 54.80409914652507\n",
        "- Epoch : 12, Loss : 33.627891385631195, Val_loss : 48.48928497411028\n",
        "- Epoch : 13, Loss : 31.059670966142303, Val_loss : 51.898619048203095\n",
        "- Epoch : 14, Loss : 28.95467392198599, Val_loss : 48.438780072369156\n",
        "- Epoch : 15, Loss : 26.666760903255195, Val_loss : 49.81154549272755\n",
        "- Epoch : 16, Loss : 24.51864461989919, Val_loss : 43.89916703067249"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uA_5jDFh7Z2L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7b9aad6db5e44a7293c6ef629d456df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6f09a8808704103b7bf553ec4e5f680",
              "IPY_MODEL_1104a548176848beba648390d91b744a",
              "IPY_MODEL_93bf54e48f6542c3961699d9e3b7ec43"
            ],
            "layout": "IPY_MODEL_f531ac6be7f44c709ae8712e2adee8d3"
          }
        },
        "b6f09a8808704103b7bf553ec4e5f680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a9841e982b6468e97be7691f1189d08",
            "placeholder": "​",
            "style": "IPY_MODEL_4380e2583bce4f21bfb861be063599c4",
            "value": "100%"
          }
        },
        "1104a548176848beba648390d91b744a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c806d0320d0e45b2b23f7c4c0c496f89",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e19f958bf8124f348e4242aababe21f6",
            "value": 111898327
          }
        },
        "93bf54e48f6542c3961699d9e3b7ec43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8959167962ff4978b0ff74c83447b870",
            "placeholder": "​",
            "style": "IPY_MODEL_39a9da5ab0bf4580bd7db989b2052d02",
            "value": " 107M/107M [00:00&lt;00:00, 229MB/s]"
          }
        },
        "f531ac6be7f44c709ae8712e2adee8d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a9841e982b6468e97be7691f1189d08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4380e2583bce4f21bfb861be063599c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c806d0320d0e45b2b23f7c4c0c496f89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e19f958bf8124f348e4242aababe21f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8959167962ff4978b0ff74c83447b870": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39a9da5ab0bf4580bd7db989b2052d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}